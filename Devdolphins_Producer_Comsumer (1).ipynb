{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fc02fb-f294-4caf-90be-e0165a56705e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Importing the Libraries required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8d3056-6b3d-4637-9d28-a599934c93ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from io import StringIO\n",
    "from datetime import datetime as t\n",
    "import time,boto3,os,psycopg2,pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "postgres_pwd = 'Sanidhya2424#'\n",
    "\n",
    "jar_path = \"dbfs:/FileStore/postgresql_42_7_6.jar\"\n",
    "sc._jsc.addJar(jar_path)\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://database-1.czy4sq8iitbm.ap-south-1.rds.amazonaws.com:5432/postgres\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": postgres_pwd,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "secret = dbutils.secrets.get(scope = \"gsrscope\", key = \"secret\")\n",
    "t_ID\t =  'a940799b-8d78-44ef-861d-12ad29d1d758' \n",
    "app_ID   =  'dae49eca-5bcd-4cb7-bcc1-b90a3c8d4a3d' \n",
    "\n",
    "storage_account_name = 'devdolphins'\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\",app_ID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{t_ID}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc720de7-4d73-4988-b14d-51ffb0f67348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Creating a dataFrame from a table which is constant throughout the process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b766393c-b068-4aa8-869f-3ceda938d9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bottom_1pct_weights_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"bottom_1pct_weights\",\n",
    "        properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb92540-99bd-45ef-b073-6f5e3b576b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Defining the functions required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab7d504-9c8c-453f-aa60-88807d5ff1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_postgres_data(df):\n",
    "\n",
    "    conn, cur = postgres_cur()\n",
    "\n",
    "    # Group by customer and merchant, aggregate transaction count and average amount\n",
    "    grouped_df = df.groupBy(col('customer'), col('merchant')).agg(\n",
    "        F.count(\"*\").alias(\"txn_count\"),\n",
    "        F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "        F.first(\"gender\").alias(\"gender\")\n",
    "    )\n",
    "\n",
    "    # Write to a staging table in PostgreSQL\n",
    "    grouped_df.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"staging_customer_merchant_txn_counts\",\n",
    "        mode=\"overwrite\",\n",
    "        properties=properties\n",
    "    )\n",
    "\n",
    "    # Upsert the data into the main table\n",
    "    upsert_sql = \"\"\"\n",
    "        INSERT INTO customer_merchant_txn_gender_counts (customer, merchant,txn_count,avg_amount,gender)\n",
    "        SELECT customer,merchant,txn_count, avg_amount,gender\n",
    "        FROM staging_customer_merchant_txn_counts\n",
    "        ON CONFLICT (customer, merchant)\n",
    "        DO UPDATE \n",
    "        SET txn_count = customer_merchant_txn_gender_counts.txn_count + EXCLUDED.txn_count,\n",
    "            avg_amount = (\n",
    "                ((customer_merchant_txn_gender_counts.avg_amount *     customer_merchant_txn_gender_counts.txn_count) + \n",
    "                 (EXCLUDED.avg_amount * EXCLUDED.txn_count)) /\n",
    "                (customer_merchant_txn_gender_counts.txn_count + EXCLUDED.txn_count)\n",
    "            );\n",
    "    \"\"\"\n",
    "    cur.execute(upsert_sql)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "    # Drop the staging table\n",
    "    cur.execute(\"DROP TABLE IF EXISTS staging_customer_merchant_txn_counts;\")\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Pattern 1: Top 1% high-transacting customers per merchant\n",
    "\n",
    "def pat1(jdbc_url, properties,df):\n",
    "    \n",
    "\n",
    "    customer_merchant_txn_counts = df\n",
    "\n",
    "   # 1. Create window for ranking\n",
    "    window_spec_by_merchant = Window.partitionBy(\"merchant\").orderBy(F.col(\"txn_count\").desc())\n",
    "\n",
    "    # 2. Percent_Rank customers by txn_count within each merchant\n",
    "    ranked_df = customer_merchant_txn_counts.withColumn(\"percentile_rank\", F.percent_rank().over(window_spec_by_merchant))\n",
    "\n",
    "    # 3. Filter top 1 percentile (percentile_rank>=0.99)\n",
    "    # top_1pct_customers_df = ranked_df.filter(col('percentile_rank')>=0.99).select(\"customer\", \"merchant\", \"txn_count\", \"txn_rank\", \"total_txn\")\n",
    "    top_1pct_customers_df = ranked_df.filter(col('percentile_rank')>=0.99).select(\"customer\", \"merchant\")\n",
    "\n",
    "    # Join with weight data\n",
    "    joined_df = top_1pct_customers_df.alias(\"t\").join(\n",
    "        bottom_1pct_weights_df.alias(\"b\"),\n",
    "        on=[\"customer\", \"merchant\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Add metadata columns\n",
    "    pattern_matches_df = joined_df.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"patternid\", F.lit(\"PatId1\"))\n",
    "    pattern_matches_df = pattern_matches_df.withColumn(\"actiontype\", F.lit(\"UPGRADE\"))\n",
    "\n",
    "    # Select final output columns\n",
    "    pattern_matches_df = pattern_matches_df.select(\n",
    "        \"ystarttime\", \"detectiontime\", \"patternid\", \"actiontype\", \"customer\", \"merchant\"\n",
    "    )\n",
    "\n",
    "    return pattern_matches_df\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Pattern 2: Child customers based on avg amount and txn count\n",
    "def pat2(jdbc_url, properties,df):\n",
    "    \n",
    "\n",
    "    child_customers_df  = df\n",
    "\n",
    "    # Identify customers with low avg amount but high txn count\n",
    "    child_customers_df = child_customers_df.filter(\n",
    "        (col(\"avg_amount\") < 23) & (col(\"txn_count\") >= 80)\n",
    "    )\n",
    "\n",
    "    # Add metadata\n",
    "    child_customers_df = child_customers_df.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    child_customers_df = child_customers_df.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    child_customers_df = child_customers_df.withColumn(\"patternid\", F.lit(\"PatId2\"))\n",
    "    child_customers_df = child_customers_df.withColumn(\"actiontype\", F.lit(\"CHILD\"))\n",
    "\n",
    "    # Select final output columns\n",
    "    child_customers_df = child_customers_df.select(\n",
    "        \"ystarttime\", \"detectiontime\", \"patternid\", \"actiontype\", \"customer\", \"merchant\"\n",
    "    )\n",
    "\n",
    "    return child_customers_df\n",
    "\n",
    "def pat3(jdbc_url, properties,df):\n",
    "\n",
    "    merchant_gender = df\n",
    "\n",
    "\n",
    "    merchant_gender = merchant_gender.groupBy(\"merchant\").agg(\n",
    "        F.sum(F.when(col(\"gender\") == \"'M'\", 1).otherwise(0)).alias(\"male\"),\n",
    "        F.sum(F.when(col(\"gender\") == \"'F'\", 1).otherwise(0)).alias(\"female\")\n",
    "    )\n",
    "\n",
    "    # merchant_gender.display()\n",
    "    # Identify merchants with gender imbalance\n",
    "    merchant_gender = merchant_gender.filter((col('male') > col('female')) & (col('female') > 100))\n",
    "\n",
    "    # Add metadata\n",
    "    merchant_gender = merchant_gender.withColumn(\"ystarttime\", F.current_timestamp())\n",
    "    merchant_gender = merchant_gender.withColumn(\"detectiontime\", F.current_timestamp())\n",
    "    merchant_gender = merchant_gender.withColumn(\"patternid\", F.lit(\"PatId3\"))\n",
    "    merchant_gender = merchant_gender.withColumn(\"actiontype\", F.lit(\"DEI-NEEDED\"))\n",
    "\n",
    "    # Final selection with customer as empty\n",
    "    merchant_gender = merchant_gender.select(\n",
    "        \"ystarttime\", \"detectiontime\", \"patternid\", \"actiontype\",\n",
    "        F.lit(\"\").alias(\"customer\"),\n",
    "        \"merchant\"\n",
    "    )\n",
    "\n",
    "    return merchant_gender\n",
    "\n",
    "# PostgreSQL connection helper\n",
    "def postgres_cur():\n",
    "    host = \"database-1.czy4sq8iitbm.ap-south-1.rds.amazonaws.com\"\n",
    "    port = \"5432\"\n",
    "    db = \"postgres\"\n",
    "    user_name = \"postgres\"\n",
    "    user_pass = postgres_pwd\n",
    "    conn = psycopg2.connect(host=host, port=port, database=db, user=user_name, password=user_pass)\n",
    "    return conn, conn.cursor()\n",
    "\n",
    "# Get next data chunk to process\n",
    "def get_next_chunk():\n",
    "    conn, cur = postgres_cur()\n",
    "    cur.execute(\"\"\"\n",
    "                    SELECT id, chunk_index, s3_path \n",
    "                    FROM chunk_tracking \n",
    "                    WHERE status = 'written' \n",
    "                    ORDER BY chunk_index ASC \n",
    "                    LIMIT 1\n",
    "                    FOR UPDATE SKIP LOCKED;\n",
    "                \"\"\")\n",
    "    data = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "# De-duplicate entries in final_output based on customer + merchant + ystarttime\n",
    "def dedupe_final_output():\n",
    "    conn, cur = postgres_cur()\n",
    "    cur.execute(\"\"\"\n",
    "                    WITH ranked_rows AS (\n",
    "                        SELECT ctid,\n",
    "                            ROW_NUMBER() OVER (PARTITION BY customer, merchant ORDER BY ystarttime ASC) AS rn\n",
    "                        FROM final_output\n",
    "                    )\n",
    "                    DELETE FROM final_output\n",
    "                    WHERE ctid IN ( SELECT ctid FROM ranked_rows WHERE rn > 1);\n",
    "                \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "# Update the status of a chunk after processing\n",
    "def update_chunk_status(chunk_index, status):\n",
    "    conn, cur = postgres_cur()\n",
    "\n",
    "    cur.execute(\"BEGIN;\")\n",
    "\n",
    "    # Lock the row first to prevent race conditions\n",
    "    cur.execute(\"\"\"\n",
    "                    SELECT * FROM chunk_tracking\n",
    "                    WHERE chunk_index = %s\n",
    "                    FOR UPDATE;\n",
    "                \"\"\", (chunk_index,))\n",
    "\n",
    "    # Update the status with timestamp\n",
    "    cur.execute(\"\"\"\n",
    "                    UPDATE chunk_tracking \n",
    "                    SET status = %s, updated_at = %s \n",
    "                    WHERE chunk_index = %s;\n",
    "                \"\"\", (status, t.utcnow(), chunk_index))\n",
    "\n",
    "    cur.execute(\"COMMIT;\")\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Generate the final output to be published\n",
    "def generate_output():\n",
    "    conn, cur = postgres_cur()\n",
    "    query = \"\"\"         \n",
    "        WITH to_publish AS (\n",
    "            SELECT merchant, customer\n",
    "            FROM final_output\n",
    "            WHERE published IS NULL\n",
    "            ORDER BY ystarttime\n",
    "            LIMIT 50\n",
    "            FOR UPDATE SKIP LOCKED\n",
    "        ),\n",
    "        check_count AS (\n",
    "            SELECT COUNT(*) AS cnt FROM to_publish\n",
    "        ),\n",
    "        update_rows AS (\n",
    "            UPDATE final_output\n",
    "            SET published = 'published'\n",
    "            WHERE (merchant, customer) IN (\n",
    "                SELECT merchant, customer FROM to_publish\n",
    "            )\n",
    "            AND EXISTS (\n",
    "                SELECT 1 FROM check_count WHERE cnt = 50\n",
    "            )\n",
    "            RETURNING *\n",
    "        )\n",
    "        SELECT * FROM update_rows;\n",
    "    \"\"\"\n",
    "    cur.execute(\"BEGIN;\")\n",
    "    cur.execute(query)\n",
    "    published_rows = cur.fetchall()\n",
    "    cur.execute(\"COMMIT;\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"ystarttime\", TimestampType(), True),\n",
    "        StructField(\"detectiontime\", TimestampType(), True),\n",
    "        StructField(\"patternid\", StringType(), True),\n",
    "        StructField(\"actiontype\", StringType(), True),\n",
    "        StructField(\"customer\", StringType(), True),\n",
    "        StructField(\"merchant\", StringType(), True),\n",
    "        StructField(\"published\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Convert to Spark DataFrame\n",
    "    df = spark.createDataFrame(published_rows, schema=schema)\n",
    "    df = df.select(\n",
    "        col(\"ystarttime\"),\n",
    "        col(\"detectiontime\"),\n",
    "        col(\"patternid\"),\n",
    "        col(\"actiontype\"),\n",
    "        col(\"customer\"),\n",
    "        col(\"merchant\")\n",
    "    )\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def get_filename(storage_account: str, container: str, folder_path: str) -> str:\n",
    "    \n",
    "    base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{folder_path}\"\n",
    "    \n",
    "    files = dbutils.fs.ls(base_path)\n",
    "    \n",
    "    # Filter for .csv files only\n",
    "    csv_files = [f.path for f in files if f.path.endswith(\".csv\")]\n",
    "    \n",
    "    if len(csv_files) != 1:\n",
    "        raise Exception(f\"Expected exactly 1 CSV file in {base_path}, but found {len(csv_files)}\")\n",
    "    \n",
    "    return csv_files[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff3b659-cfcb-4a5a-8965-b55c60df7eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Producer Process that loads data from transactions.csv and load to container incrementally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d789854-d0dc-4bfd-92b3-cffeab2840be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_0\n[2/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_1\n[3/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_2\n[4/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_3\n[5/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_4\n[6/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_5\n[7/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_6\n[8/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_7\n[9/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_8\n[10/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_9\n[11/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_10\n[12/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_11\n[13/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_12\n[14/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_13\n[15/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_14\n[16/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_15\n[17/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_16\n[18/60] Written to container and logged to DB: abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_17\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:464)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:51)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:51)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:51)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:51)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:553)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:846)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:872)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:871)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:926)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:719)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:464)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:571)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:51)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:51)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:51)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:51)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:553)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:846)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:872)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:871)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:926)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:719)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "input = f\"transactions.csv\"   # Change to your CSV file path\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "\n",
    "conn,cur = postgres_cur()\n",
    "\n",
    "# Load CSV\n",
    "df = spark.read.format('csv').option('header',True).option('inferSchema',True).load(f'abfss://source@devdolphins.dfs.core.windows.net/{input}')\n",
    "\n",
    "\n",
    "\n",
    "# Adding Index\n",
    "df_with_index = df.rdd.zipWithIndex().map(lambda row_index: Row(**row_index[0].asDict(), row_id=row_index[1]))\n",
    "df_indexed = spark.createDataFrame(df_with_index)\n",
    "\n",
    "# Total row count\n",
    "total_rows = df_indexed.count()\n",
    "total_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Loop through chunks\n",
    "for i in range(total_chunks):\n",
    "    start = i * chunk_size\n",
    "    end  = start + chunk_size\n",
    "\n",
    "    # chunk_df = limit(chunk_size + offset).subtract(df.limit(offset))\n",
    "\n",
    "    chunk_df = df_indexed.filter((col(\"row_id\")>=start) & (col(\"row_id\")<end)).drop('row_id')\n",
    "    record_count = chunk_df.count()\n",
    "\n",
    "    # Write to container\n",
    "    output_path = f\"abfss://raw@devdolphins.dfs.core.windows.net/partitions/chunk_{i}\"\n",
    "    chunk_df.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_path)\n",
    "\n",
    "    # Insert into PostgreSQL\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "                            INSERT INTO chunk_tracking (chunk_index, s3_path, record_count, status, created_at)\n",
    "                            VALUES (%s, %s, %s, %s, %s)\n",
    "                    \"\"\",(i, output_path, record_count, 'written',t.utcnow()))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"[{i+1}/{total_chunks}] Written to container and logged to DB: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DB insert failed for chunk {i}: {e}\")\n",
    "        conn.rollback()\n",
    "    # time.sleep(sleep_seconds)\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba357c4-2b2b-465c-a24c-76644a63097e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Consumer Process to consume data from lake sequentially and detect the patterns **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f9bb19-89b5-4213-8bf4-04fbaaacfb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PostgreSQL setup\n",
    "pg_conn,pg_cursor = postgres_cur()\n",
    "output_index = 0\n",
    "\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        \n",
    "        chunk = get_next_chunk()\n",
    "        if not chunk:\n",
    "            print(\"No more chunks to process.\")\n",
    "            break\n",
    "        \n",
    "        chunk_id, chunk_index, s3_path = chunk\n",
    "        print(f\"Picked chunk {chunk_index} from {s3_path}\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            try:\n",
    "                file_name = get_filename('devdolphins','raw',f'partitions/chunk_{chunk_index}')\n",
    "                df = spark.read.format('csv').option('header',True).option('inferSchema',True).load(file_name)\n",
    "            except Exception as e:\n",
    "                print(f'Can\\'t load DF {chunk_index}:{e}')\n",
    "\n",
    "            try:\n",
    "                update_postgres_data(df)\n",
    "                df = spark.read.jdbc(\n",
    "                                        url=jdbc_url,\n",
    "                                        table=\"customer_merchant_txn_gender_counts\",\n",
    "                                        properties=properties\n",
    "                                    )\n",
    "\n",
    "                pat1_df = pat1(jdbc_url,properties,df)\n",
    "                pat2_df = pat2(jdbc_url,properties,df)\n",
    "                pat3_df = pat3(jdbc_url,properties,df)\n",
    "\n",
    "                # union_df = pat2_df.unionByName(pat3_df)\n",
    "                union_df = pat1_df.unionByName(pat2_df).unionByName(pat3_df)\n",
    "\n",
    "            \n",
    "                union_df.write.jdbc(url=jdbc_url,table=\"final_output\", mode=\"overwrite\",properties=properties)\n",
    "\n",
    "               \n",
    "                output_df = generate_output()\n",
    "                if output_df.count() ==50:\n",
    "                    output_index = output_index + 1 \n",
    "                    output_path = f\"abfss://processed@devdolphins.dfs.core.windows.net/chunks/output_{output_index}\"\n",
    "                    chunk_df.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_path)\n",
    "                    print(f'output written {output_index}')\n",
    "\n",
    "                update_chunk_status(chunk_index, 'processed')\n",
    "                print(f\"Chunk {chunk_index} processed.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'Processing failed {chunk_index}:{e}')\n",
    "                update_chunk_status(chunk_index, 'failed')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_index}: {e}\")\n",
    "            update_chunk_status(chunk_index, 'failed')\n",
    "\n",
    "finally:\n",
    "    pg_cursor.close()\n",
    "    pg_conn.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Devdolphins_Producer_Comsumer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}